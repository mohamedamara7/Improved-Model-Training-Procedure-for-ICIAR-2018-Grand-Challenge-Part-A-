{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":5045636,"sourceType":"datasetVersion","datasetId":2929186}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:21:13.186033Z","iopub.execute_input":"2025-04-01T18:21:13.186342Z","iopub.status.idle":"2025-04-01T18:21:33.686839Z","shell.execute_reply.started":"2025-04-01T18:21:13.186306Z","shell.execute_reply":"2025-04-01T18:21:33.685597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:21:33.687656Z","iopub.execute_input":"2025-04-01T18:21:33.688059Z","iopub.status.idle":"2025-04-01T18:21:42.076676Z","shell.execute_reply.started":"2025-04-01T18:21:33.688036Z","shell.execute_reply":"2025-04-01T18:21:42.075824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install patchify","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:21:42.077538Z","iopub.execute_input":"2025-04-01T18:21:42.077743Z","iopub.status.idle":"2025-04-01T18:22:08.141472Z","shell.execute_reply.started":"2025-04-01T18:21:42.077723Z","shell.execute_reply":"2025-04-01T18:22:08.139867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import standard dependencies\nimport random\nimport math\nimport os\nimport math\nimport numpy as np\nimport cv2\nfrom collections import Counter\nimport re\nfrom patchify import patchify\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold, KFold, train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# image preprocessing\nfrom tensorflow.keras.preprocessing.image import img_to_array, ImageDataGenerator, load_img\n\n# to build model\nfrom keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Resizing, Rescaling, Input, Conv2D, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Lambda, Layer, Flatten\nfrom tensorflow.keras.activations import softmax\n\n# cost function / optimizer\nfrom tensorflow.keras.optimizers import SGD, Adamax\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:22:08.142550Z","iopub.execute_input":"2025-04-01T18:22:08.142817Z","iopub.status.idle":"2025-04-01T18:22:12.002373Z","shell.execute_reply.started":"2025-04-01T18:22:08.142791Z","shell.execute_reply":"2025-04-01T18:22:12.001286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SEED = 178\ntf.keras.utils.set_random_seed(SEED)\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:22:12.009357Z","iopub.execute_input":"2025-04-01T18:22:12.009675Z","iopub.status.idle":"2025-04-01T18:22:12.172510Z","shell.execute_reply.started":"2025-04-01T18:22:12.009654Z","shell.execute_reply":"2025-04-01T18:22:12.171577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAIN_DIR = \"/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos\"\nOUTPUT_DIR = \"/kaggle/working/processed_images_train/\"  # Directory for saving processed images\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nnum_classes = 4\none_hot = True\nimg_size = (224, 224)\n# Define the number of images per category for test\nimages_per_category = 10\n\nCategories = [\"Benign\", \"InSitu\", \"Invasive\", \"Normal\"]\nif one_hot:\n    encoding = {'Normal': [1,0,0,0], 'Benign': [0,1,0,0], 'InSitu': [0,0,1,0], 'Invasive': [0,0,0,1]}\nelse:    \n    encoding = {'Normal': 0, 'Benign': 1, 'InSitu': 2, 'Invasive': 3}\n\n\n# Lists to store paths of original images and labels\noriginal_images_paths = []\noriginal_images_labels = []\n\n# Lists to store paths of eval images and labels\neval_images_paths = []\neval_images_labels = []\n\n# Lists to store paths of processed images and labelsd\nprocessed_images_paths = []\nprocessed_images_labels = []\n\n# Iterate over each category and image\nfor category in Categories:\n    category_dir = os.path.join(MAIN_DIR, category)\n    number_of_test = images_per_category\n    for image_name in os.listdir(category_dir):\n        if image_name.endswith(\".tif\"):\n            image_path = os.path.join(category_dir, image_name)\n            label = encoding[category]\n            \n            original_images_paths.append(image_path)\n            original_images_labels.append(label)\n            \n            if number_of_test:\n                number_of_test = number_of_test - 1\n                eval_images_paths.append(image_path)\n                eval_images_labels.append(label)\n            else:\n                # Open and process the image using OpenCV\n                img = cv2.imread(image_path)\n                patches = patchify(img,patch_size=(1400,1400,3), step=92)\n                for i in range(patches.shape[0]):\n                    for j in range(patches.shape[1]):\n                        img_resized = cv2.resize(patches[i, j, 0, :, :, :], img_size, interpolation=cv2.INTER_LANCZOS4)\n                        filenames = [f'{OUTPUT_DIR}{category}_{image_name[:-4]}_{i:02d}_{j:02d}_{k}.tif' for k in range(6)]\n                        processed_images_paths=processed_images_paths+filenames\n                        cv2.imwrite(filenames[0], img_resized, [cv2.IMWRITE_TIFF_COMPRESSION, 1]) # orignal\n                        cv2.imwrite(filenames[1], cv2.flip(img_resized,0), [cv2.IMWRITE_TIFF_COMPRESSION, 1]) # flip up down\n                        cv2.imwrite(filenames[2], cv2.flip(img_resized,1), [cv2.IMWRITE_TIFF_COMPRESSION, 1]) # flip left right\n                        cv2.imwrite(filenames[3], cv2.rotate(img_resized,0), [cv2.IMWRITE_TIFF_COMPRESSION, 1]) # rotate +90\n                        cv2.imwrite(filenames[4], cv2.rotate(img_resized,1), [cv2.IMWRITE_TIFF_COMPRESSION, 1]) # rotate +180\n                        cv2.imwrite(filenames[5], cv2.rotate(img_resized,2), [cv2.IMWRITE_TIFF_COMPRESSION, 1]) # rotate +270\n                        processed_images_labels=processed_images_labels+[label]*6\n\n# Converting to NumPy arrays for cross-validation\nprocessed_images_paths = np.array(processed_images_paths)\nprocessed_images_labels = np.array(processed_images_labels)\noriginal_images_paths = np.array(original_images_paths)\noriginal_images_labels = np.array(original_images_labels)\neval_images_paths = np.array(eval_images_paths)\neval_images_labels = np.array(eval_images_labels)\n# Display the count of processed and original images and labels\nprint(\"Number of processed images:\", len(processed_images_paths))\nprint(\"Number of processed labels:\", len(processed_images_labels))\nprint(\"Number of original images:\", len(original_images_paths))\nprint(\"Number of original labels:\", len(original_images_labels))\nprint(\"Number of test images:\", len(eval_images_paths))\nprint(\"Number of test labels:\", len(eval_images_labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:22:12.173808Z","iopub.execute_input":"2025-04-01T18:22:12.174034Z","iopub.status.idle":"2025-04-01T18:25:12.467291Z","shell.execute_reply.started":"2025-04-01T18:22:12.174013Z","shell.execute_reply":"2025-04-01T18:25:12.466222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TEST_DIR = \"/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge_TestDataset/ICIAR2018_BACH_Challenge_TestDataset/Photos\"\nOUTPUT_DIR = \"/kaggle/working/processed_images_test/\"  # Directory for saving processed images\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nimg_size_test = (224, 224)\n\n# Lists to store paths of processed images and labels\ntest_images_paths = []\ntest_images_numbers = []\n\ntest_patch_images_paths = []\ntest_patch_images_numbers = []\nfor image_name in os.listdir(TEST_DIR):\n    if image_name.endswith(\".tif\"):\n        image_path = os.path.join(TEST_DIR, image_name)\n        test_images_paths.append(image_path)  \n        match = re.search(r'test(\\d+)\\.tif', image_name)\n        image_number = int(match.group(1))  # Extracted number as integer\n        test_images_numbers.append(image_number)\n        \n        img = cv2.imread(image_path)\n        patches = patchify(img,patch_size=(1400,1400,3), step=92)\n        for i in range(patches.shape[0]):\n            for j in range(patches.shape[1]):\n                img_resized = cv2.resize(patches[i, j, 0, :, :, :], img_size_test, interpolation=cv2.INTER_LANCZOS4)\n                img_name = f'{OUTPUT_DIR}{image_name[:-4]}_{i:02d}_{j:02d}.tif'\n                test_patch_images_paths.append(img_name)\n                cv2.imwrite(img_name, img_resized, [cv2.IMWRITE_TIFF_COMPRESSION, 1]) # orignal\n                test_patch_images_numbers.append(image_number)\n\nprint(\"Number of test images:\", len(test_images_paths))\nprint(\"Number of test ids:\", len(test_images_numbers))\n\nprint(\"Number of processed test images:\", len(test_patch_images_paths))\nprint(\"Number of processed test ids:\", len(test_patch_images_numbers))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:13.527519Z","iopub.execute_input":"2025-04-01T18:25:13.527898Z","iopub.status.idle":"2025-04-01T18:25:55.022318Z","shell.execute_reply.started":"2025-04-01T18:25:13.527873Z","shell.execute_reply":"2025-04-01T18:25:55.021264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imgShape = (224, 224, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.023111Z","iopub.execute_input":"2025-04-01T18:25:55.023378Z","iopub.status.idle":"2025-04-01T18:25:55.026689Z","shell.execute_reply.started":"2025-04-01T18:25:55.023355Z","shell.execute_reply":"2025-04-01T18:25:55.025957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_image(img_path, label=None, resize=False, normalize=False):\n    def load_and_preprocess_image(img_path):\n        # Convert the img_path from tensor to string\n        img_path = img_path.numpy().decode('UTF-8')\n        # Read the image file using OpenCV\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        # Convert from BGR to RGB (OpenCV default is BGR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Optionally resize the image\n        if resize:\n            img = cv2.resize(img, (imgShape[1], imgShape[0]))\n\n        # Optionally normalize the image to the range [-1, 1]\n        if normalize:\n            img = img / 127.5 - 1.0\n\n        return img.astype(np.float32)\n\n    # Use tf.py_function to wrap the Python function\n    img = tf.py_function(load_and_preprocess_image, [img_path], tf.float32)\n    # Set the shape of the image after modification by the tf.py_function\n    img.set_shape(imgShape)\n\n    # Return img and label if label is provided\n    return (img, tf.cast(label,tf.float32)) if label is not None else img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.027660Z","iopub.execute_input":"2025-04-01T18:25:55.027877Z","iopub.status.idle":"2025-04-01T18:25:55.042970Z","shell.execute_reply.started":"2025-04-01T18:25:55.027855Z","shell.execute_reply":"2025-04-01T18:25:55.042141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.043872Z","iopub.execute_input":"2025-04-01T18:25:55.044059Z","iopub.status.idle":"2025-04-01T18:25:55.056287Z","shell.execute_reply.started":"2025-04-01T18:25:55.044039Z","shell.execute_reply":"2025-04-01T18:25:55.055525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"orig_ds = tf.data.Dataset.from_tensor_slices((original_images_paths, original_images_labels))\n\neval_ds = None \nif images_per_category:\n    eval_ds = tf.data.Dataset.from_tensor_slices((eval_images_paths, eval_images_labels))\n    eval_ds = eval_ds.map(lambda img, label: preprocess_image(img, label, resize=True), num_parallel_calls=AUTOTUNE)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((test_images_paths, test_images_numbers))\npatch_test_ds = tf.data.Dataset.from_tensor_slices((test_patch_images_paths, test_patch_images_numbers))\n\n# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\norig_ds = orig_ds.map(lambda img, label: preprocess_image(img, label, resize=True), num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.map(lambda img, number: preprocess_image(img, number, resize=True), num_parallel_calls=AUTOTUNE)\npatch_test_ds = patch_test_ds.map(lambda img, number: preprocess_image(img, number), num_parallel_calls=AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.057197Z","iopub.execute_input":"2025-04-01T18:25:55.057399Z","iopub.status.idle":"2025-04-01T18:25:55.256343Z","shell.execute_reply.started":"2025-04-01T18:25:55.057380Z","shell.execute_reply":"2025-04-01T18:25:55.255528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TrivialAugmentWide(tf.keras.layers.Layer):\n    def __init__(self, num_magnitude_bins=31, interpolation='nearest', fill=None, exclude_ops=None, **kwargs):\n        super().__init__(**kwargs)\n        self.num_magnitude_bins = num_magnitude_bins\n        self.interpolation = interpolation\n        self.fill = fill\n        self.exclude_ops = exclude_ops if exclude_ops else []\n        self.ops_space = self._augmentation_space(num_magnitude_bins)\n        self.AFFINE_TRANSFORM_INTERPOLATIONS = (\"nearest\",\"bilinear\")\n        self.AFFINE_TRANSFORM_FILL_MODES = (\"constant\",\"nearest\",\"wrap\",\"reflect\")\n\n        # Filter out excluded operations and create TensorFlow-compatible structures\n        self.op_names = tf.constant([name for name in self.ops_space.keys() if name not in self.exclude_ops])\n        \n        # Ensure all magnitudes are cast to float32 and padded to match the shape [num_magnitude_bins]\n        def pad_magnitude(magnitude):\n            if magnitude.shape.rank == 0:  # Scalar tensor\n                return tf.fill([self.num_magnitude_bins], magnitude)\n            return magnitude\n\n        self.op_magnitudes = tf.stack([\n            pad_magnitude(tf.cast(params[0], tf.float32))\n            for name, params in self.ops_space.items() if name not in self.exclude_ops\n        ])\n        \n        # Signed flags remain as a boolean tensor\n        self.op_signed = tf.constant([params[1] for name, params in self.ops_space.items() if name not in self.exclude_ops])\n\n    def _augmentation_space(self, num_bins):\n        return {\n            \"Identity\": (tf.constant(0.0), False),\n            \"ShearX\": (tf.linspace(0.0, 0.99, num_bins), True),\n            \"ShearY\": (tf.linspace(0.0, 0.99, num_bins), True),\n            \"TranslateX\": (tf.linspace(0.0, 32.0, num_bins), True),\n            \"TranslateY\": (tf.linspace(0.0, 32.0, num_bins), True),\n            \"Rotate\": (tf.linspace(0.0, 135.0, num_bins), True),\n            \"Brightness\": (tf.linspace(0.0, 0.99, num_bins), True),\n            \"Color\": (tf.linspace(0.0, 0.99, num_bins), True),\n            \"Contrast\": (tf.linspace(0.0, 0.99, num_bins), True),\n            \"Sharpness\": (tf.linspace(0.0, 0.99, num_bins), True),\n            \"Posterize\": ( tf.cast(8 - tf.round(tf.cast(tf.range(num_bins), tf.float32) / ((num_bins - 1) / 6)), tf.uint8), False, ),\n            \"Solarize\": (tf.linspace(255.0, 0.0, num_bins), False),\n            \"AutoContrast\": (tf.constant(0.0), False),\n            \"Equalize\": (tf.constant(0.0), False),\n        }\n\n    def _augment_one(self, img):\n        # Ensure input is in [0, 255] range and uint8 type\n        input_image_type = img.dtype\n        if img.dtype != tf.uint8:\n            img = tf.cast(tf.clip_by_value(img, 0, 255), tf.uint8)\n        \n        # Use a default fill value if self.fill is None\n        fill = self.fill if self.fill is not None else 0.0\n        \n        # Randomly select an operation\n        op_index = tf.random.uniform(shape=(), maxval=tf.shape(self.op_names)[0], dtype=tf.int32)\n        op_name = tf.gather(self.op_names, op_index)\n        magnitudes = tf.gather(self.op_magnitudes, op_index)\n        signed = tf.gather(self.op_signed, op_index)\n        \n        # Randomly select a magnitude using tf.cond to handle symbolic rank\n        magnitude = tf.cond(\n            tf.greater(tf.rank(magnitudes), 0),\n            lambda: tf.gather(magnitudes, tf.random.uniform(shape=(), maxval=tf.shape(magnitudes)[0], dtype=tf.int32)),\n            lambda: tf.constant(0.0, dtype=tf.float32)\n        )\n        \n        # Apply random sign if required\n        magnitude = tf.cond(\n            signed,\n            lambda: magnitude * tf.cond(tf.random.uniform(shape=()) > 0.5, lambda: 1.0, lambda: -1.0),\n            lambda: magnitude\n        )\n        \n        # Apply the chosen operation\n        img = self._apply_op(img, op_name, magnitude, fill=fill)\n        \n        # Final clip and cast back to uint8\n        return tf.cast(img, dtype=input_image_type)\n\n    @tf.function\n    def call(self, img):\n        # Check if the rank is statically known\n        if img.shape.rank is not None:\n            if img.shape.rank == 4:\n                return tf.map_fn(self._augment_one, img)\n            else:\n                return self._augment_one(img)\n        else:\n            # When rank is unknown, use tf.cond with a symbolic comparison.\n            return tf.cond(\n                tf.equal(tf.rank(img), 4),\n                lambda: tf.map_fn(self._augment_one, img),\n                lambda: self._augment_one(img)\n            )\n\n    def _apply_op(self, img, op_name, magnitude, fill):\n        # Define a mapping from operation names to their corresponding functions\n        def apply_shear_x(): return self.shear_x(img, magnitude)\n        def apply_shear_y(): return self.shear_y(img, magnitude)\n        def apply_translate_x(): return self.translate_x(img, magnitude)\n        def apply_translate_y(): return self.translate_y(img, magnitude)\n        def apply_rotate(): return self.rotate(img, magnitude)\n        def apply_brightness(): return self.adjust_brightness(img, magnitude + 1.0)\n        def apply_color(): return self.adjust_saturation(img, magnitude + 1.0)\n        def apply_contrast(): return self.adjust_contrast(img, magnitude + 1.0)\n        def apply_sharpness(): return self.adjust_sharpness(img, magnitude + 1.0)\n        def apply_posterize(): return self.posterize(img, magnitude)\n        def apply_solarize(): return self.solarize(img, magnitude)\n        def apply_autocontrast(): return self.autocontrast(img)\n        def apply_equalize(): return self.equalize(img)\n        def apply_identity(): return img\n\n        # Use tf.case to select the appropriate operation based on op_name\n        img = tf.switch_case(\n            branch_index=tf.cast(tf.argmax(tf.equal(op_name, tf.constant([\n                \"ShearX\", \"ShearY\", \"TranslateX\", \"TranslateY\", \"Rotate\",\n                \"Brightness\", \"Color\", \"Contrast\", \"Sharpness\", \"Posterize\",\n                \"Solarize\", \"AutoContrast\", \"Equalize\", \"Identity\"\n            ]))), tf.int32),\n            branch_fns={\n                0: apply_shear_x,\n                1: apply_shear_y,\n                2: apply_translate_x,\n                3: apply_translate_y,\n                4: apply_rotate,\n                5: apply_brightness,\n                6: apply_color,\n                7: apply_contrast,\n                8: apply_sharpness,\n                9: apply_posterize,\n                10: apply_solarize,\n                11: apply_autocontrast,\n                12: apply_equalize,\n                13: apply_identity\n            },\n            default=apply_identity\n        )\n        return img\n\n    def blend(self, image1, image2, factor):\n        image1 = tf.cast(image1, tf.float32)\n        image2 = tf.cast(image2, tf.float32)\n        factor = tf.cast(factor, tf.float32)\n\n        difference = image2 - image1\n        scaled = factor * difference\n\n        # Do addition in float.\n        temp = image1 + scaled\n        return tf.cast(tf.clip_by_value(temp, 0.0, 255.0), tf.uint8)\n\n    # Affine Transformation Helper\n    def _affine_transform(self, img, transform_matrix):\n        a0, a1, a2 = transform_matrix[0], transform_matrix[1], transform_matrix[2]\n        b0, b1, b2 = transform_matrix[3], transform_matrix[4], transform_matrix[5]\n\n        # Create the 8-parameter transform tensor\n        transforms = tf.stack([a0, a1, a2, b0, b1, b2, 0.0, 0.0])\n        transforms = tf.reshape(transforms, [1, 8])  # Shape [1, 8]\n\n        # Convert image to float32 and add batch dimension\n        img_float = tf.cast(img, tf.float32)\n        img_batched = tf.expand_dims(img_float, axis=0)\n\n        # Apply affine transform using Keras ops\n        transformed = tf.keras.ops.image.affine_transform(\n            img_batched,\n            transforms,\n            interpolation='bilinear',\n            fill_mode='constant',\n            fill_value=self.fill if self.fill is not None else 0.0\n        )\n        # Remove batch dimension and convert back to uint8\n        transformed = tf.squeeze(transformed, axis=0)\n        return tf.cast(tf.clip_by_value(transformed, 0, 255), tf.uint8)\n\n    def shear_x(self, img, magnitude):\n        transform_matrix = tf.stack([\n            1.0, magnitude, 0.0,  # a0, a1, a2\n            0.0, 1.0, 0.0        # b0, b1, b2\n        ])\n        return self._affine_transform(img, transform_matrix)\n\n    def shear_y(self, img, magnitude):\n        transform_matrix = tf.stack([\n            1.0, 0.0, 0.0,       # a0, a1, a2\n            magnitude, 1.0, 0.0   # b0, b1, b2\n        ])\n        return self._affine_transform(img, transform_matrix)\n\n    def translate_x(self, img, magnitude):\n        transform_matrix = tf.stack([\n            1.0, 0.0, magnitude,  # a0, a1, a2\n            0.0, 1.0, 0.0         # b0, b1, b2\n        ])\n        return self._affine_transform(img, transform_matrix)\n\n    def translate_y(self, img, magnitude):\n        transform_matrix = tf.stack([\n            1.0, 0.0, 0.0,       # a0, a1, a2\n            0.0, 1.0, magnitude  # b0, b1, b2\n        ])\n        return self._affine_transform(img, transform_matrix)\n\n    def rotate(self, img, angle):\n        angle_rad = -math.pi * angle / 180.0\n        cos_a = tf.cos(angle_rad)\n        sin_a = tf.sin(angle_rad)\n\n        # Center of rotation\n        h, w = tf.shape(img)[0], tf.shape(img)[1]\n        cx, cy = tf.cast(w, tf.float32) / 2.0, tf.cast(h, tf.float32) / 2.0\n\n        transform_matrix = tf.stack([\n            cos_a, -sin_a, (1 - cos_a) * cx + sin_a * cy,  # a0, a1, a2\n            sin_a, cos_a, -sin_a * cx + (1 - cos_a) * cy   # b0, b1, b2\n        ])\n        return self._affine_transform(img, transform_matrix)\n\n    # Adjust Brightness\n    def adjust_brightness(self, img, brightness_factor):\n        degenerate = tf.zeros_like(img)\n        return self.blend(degenerate, img, brightness_factor)\n\n    # Adjust Contrast\n    def adjust_contrast(self, img, contrast_factor):\n        # degenerate = tf.image.rgb_to_grayscale(img)\n        # # Cast before calling tf.histogram.\n        # degenerate = tf.cast(degenerate, tf.int32)\n\n        # # Compute the grayscale histogram, then compute the mean pixel value,\n        # # and create a constant image size of that value.  Use that as the\n        # # blending degenerate target of the original image.\n        # hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n        # mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.0\n        # degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n        # degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n        # degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n        degenerate = tf.math.reduce_mean(tf.image.rgb_to_grayscale(img), axis=[-3, -2, -1], keepdims=True)\n        return self.blend(degenerate, img, contrast_factor)\n\n    # Adjust Saturation\n    def adjust_saturation(self, img, saturation_factor):\n        degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(img))\n        return self.blend(degenerate, img, saturation_factor)\n\n    # Posterize\n    def posterize(self, img, bits):\n        bits = tf.cast(bits, img.dtype)\n        shift = 8 - bits\n        return tf.bitwise.left_shift(tf.bitwise.right_shift(img, shift), shift)\n\n    # Solarize\n    def solarize(self, img, threshold):\n        threshold = tf.cast(threshold, img.dtype)\n        return tf.where(img >= threshold, 255 - img, img)\n\n    # Autocontrast\n    def autocontrast(self, img):\n        def scale_channel(image: tf.Tensor) -> tf.Tensor:\n          \"\"\"Scale the 2D image using the autocontrast rule.\"\"\"\n          # A possibly cheaper version can be done using cumsum/unique_with_counts\n          # over the histogram values, rather than iterating over the entire image.\n          # to compute mins and maxes.\n          lo = tf.cast(tf.reduce_min(image), tf.float32)\n          hi = tf.cast(tf.reduce_max(image), tf.float32)\n\n          # Scale the image, making the lowest value 0 and the highest value 255.\n          def scale_values(im):\n            scale = 255.0 / (hi - lo)\n            offset = -lo * scale\n            im = tf.cast(im, tf.float32) * scale + offset\n            im = tf.clip_by_value(im, 0.0, 255.0)\n            return tf.cast(im, tf.uint8)\n\n          result = tf.cond(hi > lo, lambda: scale_values(image), lambda: image)\n          return result\n\n        # Assumes RGB for now.  Scales each channel independently\n        # and then stacks the result.\n        s1 = scale_channel(img[..., 0])\n        s2 = scale_channel(img[..., 1])\n        s3 = scale_channel(img[..., 2])\n        img = tf.stack([s1, s2, s3], -1)\n\n        return img\n\n    # Equalize\n    def equalize(self, image):\n        \"\"\"Implements Equalize function from PIL using TF ops.\"\"\"\n\n        def scale_channel(im, c):\n          \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n          im = tf.cast(im[..., c], tf.int32)\n          # Compute the histogram of the image channel.\n          histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n\n          # For the purposes of computing the step, filter out the nonzeros.\n          nonzero = tf.where(tf.not_equal(histo, 0))\n          nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n          step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n\n          def build_lut(histo, step):\n            # Compute the cumulative sum, shifting by step // 2\n            # and then normalization by step.\n            lut = (tf.cumsum(histo) + (step // 2)) // step\n            # Shift lut, prepending with 0.\n            lut = tf.concat([[0], lut[:-1]], 0)\n            # Clip the counts to be in range.  This is done\n            # in the C code for image.point.\n            return tf.clip_by_value(lut, 0, 255)\n\n          # If step is zero, return the original image.  Otherwise, build\n          # lut from the full histogram and step and then index from it.\n          result = tf.cond(\n              tf.equal(step, 0), lambda: im,\n              lambda: tf.gather(build_lut(histo, step), im))\n\n          return tf.cast(result, tf.uint8)\n\n        # Assumes RGB for now.  Scales each channel independently\n        # and then stacks the result.\n        s1 = scale_channel(image, 0)\n        s2 = scale_channel(image, 1)\n        s3 = scale_channel(image, 2)\n        image = tf.stack([s1, s2, s3], -1)\n        return image\n\n    def adjust_sharpness(self, image, factor):\n        orig_image = image\n        image = tf.cast(image, tf.float32)\n        # Make image 4D for conv operation.\n        image = tf.expand_dims(image, 0)\n        # SMOOTH PIL Kernel.\n        kernel = tf.constant([[1, 1, 1], [1, 5, 1], [1, 1, 1]],\n                              dtype=tf.float32,\n                              shape=[3, 3, 1, 1]) / 13.\n        # Tile across channel dimension.\n        kernel = tf.tile(kernel, [1, 1, 3, 1])\n        strides = [1, 1, 1, 1]\n        degenerate = tf.nn.depthwise_conv2d(\n            image, kernel, strides, padding='VALID', dilations=[1, 1])\n        degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n        degenerate = tf.squeeze(tf.cast(degenerate, tf.uint8), [0])\n\n        # For the borders of the resulting image, fill in the values of the\n        # original image.\n        mask = tf.ones_like(degenerate)\n        paddings = [[0, 0]] * (orig_image.shape.rank - 3)\n        padded_mask = tf.pad(mask, paddings + [[1, 1], [1, 1], [0, 0]])\n        padded_degenerate = tf.pad(degenerate, paddings + [[1, 1], [1, 1], [0, 0]])\n        result = tf.where(tf.equal(padded_mask, 1), padded_degenerate, orig_image)\n\n        # Blend the final result.\n        return self.blend(result, orig_image, factor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.257410Z","iopub.execute_input":"2025-04-01T18:25:55.257654Z","iopub.status.idle":"2025-04-01T18:25:55.301338Z","shell.execute_reply.started":"2025-04-01T18:25:55.257633Z","shell.execute_reply":"2025-04-01T18:25:55.300317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nTo train a model with this dataset you will want the data:\n  To be well shuffled.\n  To be batched.\n  Batches to be available as soon as possible.\n\"\"\"\nif tpu is not None:\n    batch_size_per_replica = 64\n    batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\nelse: \n    batch_size = 8\n\nTAW = TrivialAugmentWide(exclude_ops=['Rotate'])\n\ndef configure_for_performance(ds, shuffle=False, augment=False, drop_remainder=True):\n    if shuffle:\n        ds=ds.shuffle(10000)\n    ds = ds.cache()\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    \n    if augment:\n        @tf.function\n        def augment_fn(imgs, labels):\n            # Augment images\n            aug_imgs = TAW(imgs)\n            return aug_imgs, labels\n        ds = ds.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch next batch while training\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:32:23.349193Z","iopub.execute_input":"2025-04-01T18:32:23.349566Z","iopub.status.idle":"2025-04-01T18:32:23.390959Z","shell.execute_reply.started":"2025-04-01T18:32:23.349539Z","shell.execute_reply":"2025-04-01T18:32:23.389869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"orig_ds = orig_ds.batch(batch_size, drop_remainder=False)\n\nif images_per_category:\n    eval_ds = eval_ds.batch(batch_size, drop_remainder=False)\n\ntest_ds = test_ds.batch(batch_size, drop_remainder=False)\npatch_test_ds = patch_test_ds.batch(batch_size, drop_remainder=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.382928Z","iopub.execute_input":"2025-04-01T18:25:55.383125Z","iopub.status.idle":"2025-04-01T18:25:55.390758Z","shell.execute_reply.started":"2025-04-01T18:25:55.383105Z","shell.execute_reply":"2025-04-01T18:25:55.389980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import ops\n\nEPOCHS = 70\nsteps_per_epoch = int((len(processed_images_paths)*0.8) // batch_size)\nvalidation_steps = int((len(processed_images_paths)*0.2) / batch_size)\ntotal_steps = steps_per_epoch * EPOCHS\nwarmup_steps = int(0.1 * total_steps)\nhold_steps = int(0.45 * total_steps)\n\ndef lr_warmup_cosine_decay(global_step, warmup_steps, hold=0, total_steps=0, start_lr=0.0, target_lr=1e-2):\n    # Cosine decay\n    learning_rate = (0.5*target_lr*(1+ops.cos(math.pi*ops.convert_to_tensor(global_step-warmup_steps-hold, dtype=\"float32\")/ops.convert_to_tensor(total_steps-warmup_steps-hold, dtype=\"float32\"))))\n\n    warmup_lr = target_lr * (global_step / warmup_steps)\n\n    if hold > 0:\n        learning_rate = ops.where(global_step > warmup_steps + hold, learning_rate, target_lr)\n\n    learning_rate = ops.where(global_step < warmup_steps, warmup_lr, learning_rate)\n    return learning_rate\n\nclass WarmUpCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, warmup_steps, total_steps, hold, start_lr=0.0, target_lr=1e-2):\n        super().__init__()\n        self.start_lr = start_lr\n        self.target_lr = target_lr\n        self.warmup_steps = warmup_steps\n        self.total_steps = total_steps\n        self.hold = hold\n\n    def __call__(self, step):\n        lr = lr_warmup_cosine_decay(\n            global_step=step,\n            total_steps=self.total_steps,\n            warmup_steps=self.warmup_steps,\n            start_lr=self.start_lr,\n            target_lr=self.target_lr,\n            hold=self.hold,\n        )\n        return ops.where(step > self.total_steps, 0.0, lr)\n\n\nschedule = WarmUpCosineDecay(\n    start_lr=0.001,\n    target_lr=0.0001,\n    warmup_steps=warmup_steps,\n    total_steps=total_steps,\n    hold=hold_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.391382Z","iopub.execute_input":"2025-04-01T18:25:55.391590Z","iopub.status.idle":"2025-04-01T18:25:55.404079Z","shell.execute_reply.started":"2025-04-01T18:25:55.391571Z","shell.execute_reply":"2025-04-01T18:25:55.403429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def new_tta(images):\n    \"\"\"\n    Apply Test Time Augmentation (TTA) on a batch of images.\n    \n    Args:\n        images (tf.Tensor): Input images in channels-last format, shape [B, H, W, C].\n        \n    Returns:\n        list of tf.Tensor: A list of augmented image batches, each with the same shape as the input.\n                          The list contains the original, flipped left-right, and rotated (90°, 180°, 270°) versions.\n    \"\"\"\n    ret = []\n    \n    # Add the original images\n    ret.append(images)\n    ret.append(images)\n    \n    # Flip left-right\n    flipped_lr = tf.image.flip_left_right(images)\n    ret.append(flipped_lr)\n\n    # Flip up_down\n    flipped_ud = tf.image.flip_up_down(images)\n    ret.append(flipped_ud)\n    \n    # Rotate 90°\n    rotated_90 = tf.image.rot90(images, k=1)\n    ret.append(rotated_90)\n    \n    # Rotate 180°\n    rotated_180 = tf.image.rot90(images, k=2)\n    ret.append(rotated_180)\n    \n    # Rotate 270°\n    rotated_270 = tf.image.rot90(images, k=3)\n    ret.append(rotated_270)\n    \n    return ret","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.420397Z","iopub.execute_input":"2025-04-01T18:25:55.420760Z","iopub.status.idle":"2025-04-01T18:25:55.436064Z","shell.execute_reply.started":"2025-04-01T18:25:55.420738Z","shell.execute_reply":"2025-04-01T18:25:55.435229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GeM(tf.keras.layers.Layer):\n    \"\"\"\n    Generalized Mean (GeM) pooling.\n    \"\"\"\n    def __init__(self, p=3.0, eps=1e-6, **kwargs):\n        \"\"\"\n        Args:\n            p (float): The power to raise the inputs.\n            eps (float): Small value to avoid numerical issues.\n        \"\"\"\n        super(GeM, self).__init__(**kwargs)\n        self.p = p\n        self.eps = eps\n\n    def call(self, inputs):\n        # Clip values for numerical stability.\n        x = tf.clip_by_value(inputs, self.eps, tf.reduce_max(inputs))\n        x = tf.pow(x, self.p)\n        # Global average pooling over the spatial dimensions.\n        # Assumes inputs are in channels-last format: (batch, height, width, channels)\n        x = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n        x = tf.pow(x, 1.0 / self.p)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.436892Z","iopub.execute_input":"2025-04-01T18:25:55.437122Z","iopub.status.idle":"2025-04-01T18:25:55.451212Z","shell.execute_reply.started":"2025-04-01T18:25:55.437103Z","shell.execute_reply":"2025-04-01T18:25:55.450399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ArcMarginProduct(Layer):\n    def __init__(self, units=1000, **kwargs):\n        super(ArcMarginProduct, self).__init__(**kwargs)\n        self.units = units\n        # self.kernel_regularizer\n    \n    def build(self, input_shape):\n            self.w = self.add_weight(\n                name=\"norm_dense_w\",\n                shape=(input_shape[-1], self.units),\n                initializer=tf.keras.initializers.GlorotUniform(), # GlorotNormal()\n                trainable=True\n            )\n            # super(ArcMarginProduct, self).build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        norm_w = tf.nn.l2_normalize(self.w, axis=0, epsilon=1e-5) # each column is a weight vector\n        norm_embedding = tf.nn.l2_normalize(inputs, axis=1, epsilon=1e-5)\n        cos_theta = tf.linalg.matmul(norm_embedding, norm_w, name='cos_theta')\n        return cos_theta\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.units)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.452259Z","iopub.execute_input":"2025-04-01T18:25:55.452558Z","iopub.status.idle":"2025-04-01T18:25:55.466635Z","shell.execute_reply.started":"2025-04-01T18:25:55.452537Z","shell.execute_reply":"2025-04-01T18:25:55.465459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ArcfaceLoss(tf.keras.losses.Loss):\n    def __init__(self, margin1=1.0, margin2=0.5, margin3=0.0, scale=64.0, smoothing=0.0, one_hot=True, **kwargs):\n        super(ArcfaceLoss, self).__init__(**kwargs)\n        self.margin1, self.margin2, self.margin3, self.scale = margin1, margin2, margin3, scale\n        # self.threshold = np.cos((np.pi - margin2) / margin1)  # grad(theta) == 0\n        # self.theta_margin_min = (-1 - margin3) * 2\n        self.gamma = 0.2\n        if one_hot:\n            self.loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum_over_batch_size\", from_logits = True, label_smoothing=smoothing) # Linear activation input\n        else:\n            self.loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=\"sum_over_batch_size\", from_logits = True) # Linear activation input\n        \n    def call(self, y_true, cos_theta):\n        # y_true = tf.cast(y_true,tf.float32)\n        theta = tf.math.acos(tf.keras.backend.clip(cos_theta, -1.0 + tf.keras.backend.epsilon(), 1.0 - tf.keras.backend.epsilon()))\n        target_logits = tf.cos(theta * self.margin1 + self.margin2) - self.margin3\n        logits = (cos_theta * (1.0 - y_true) + target_logits * y_true) * self.scale\n        loss1 = self.loss(y_true, logits)\n        # loss2 = self.loss(y_true, cos_theta)\n        # loss = (loss1+self.gamma*loss2)/(1+self.gamma)\n        return loss1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.467331Z","iopub.execute_input":"2025-04-01T18:25:55.467539Z","iopub.status.idle":"2025-04-01T18:25:55.477738Z","shell.execute_reply.started":"2025-04-01T18:25:55.467519Z","shell.execute_reply":"2025-04-01T18:25:55.476829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Definition\nclass Model(tf.keras.Model):\n    def __init__(self, backbone_name, num_classes, embedding_size=512):\n        super(Model, self).__init__()\n        self.backbone_name = backbone_name\n        self.num_classes = num_classes\n        self.embedding_size = embedding_size\n        \n        self.preprocess_fn = None\n        self.resize_layer = None  # tf.keras.layers.Resizing(*(224, 224))\n        # self.cust_norm = CustomNormalization(mean=[0.7165332113314071, 0.6166415441176472, 0.8421312542595161], std_dev=[0.16585887053987372, 0.18836123162530533, 0.11803739988823378])\n        self.backbone = self._initialize_extractor() # preprocess_fn now has value\n\n        self.pool = GeM()  # Default: p=3, eps=1e-6\n        self.flatten = Flatten() # flatten for GeM\n        self.fc = Dense(embedding_size, use_bias=False)\n        self.bn = BatchNormalization()\n        self.arcMargin = ArcMarginProduct(self.num_classes)\n\n    def _initialize_extractor(self):\n        if self.backbone_name == 'densenet':\n            self.preprocess_fn = tf.keras.applications.densenet.preprocess_input\n            model = tf.keras.applications.DenseNet169(include_top=False, weights=\"imagenet\")\n        elif self.backbone_name == 'resnet':\n            self.preprocess_fn = tf.keras.applications.resnet50.preprocess_input\n            model = tf.keras.applications.ResNet50V2(include_top=False, weights=\"imagenet\")\n        elif self.backbone_name == 'efficientnet':\n            self.preprocess_fn = tf.keras.applications.efficientnet.preprocess_input\n            model = tf.keras.applications.EfficientNetB2(include_top=False, weights=\"imagenet\")\n        elif self.backbone_name == 'efficientnetv2':\n            self.preprocess_fn = tf.keras.applications.efficientnet_v2.preprocess_input\n            model = tf.keras.applications.EfficientNetV2B1(include_top=False, weights=\"imagenet\")\n        elif self.backbone_name == 'xception':\n            self.preprocess_fn = tf.keras.applications.xception.preprocess_input\n            model = tf.keras.applications.Xception(include_top=False, weights=\"imagenet\")\n        elif self.backbone_name == 'inc_res':\n            self.preprocess_fn = tf.keras.applications.inception_resnet_v2.preprocess_input\n            model = tf.keras.applications.InceptionResNetV2(include_top=False, weights=\"imagenet\")\n        else:\n            raise ValueError('Invalid backbone specified')\n\n        # UnFreeze the base model layers\n        for layer in model.layers:\n            layer.trainable = True\n            \n        return model\n    \n\n    def call(self, inputs, training=False):\n        \"\"\"\n        Forward pass through the model.\n        \n        Args:\n            inputs: Input images.\n            training (bool): Whether the model is in training mode.\n        \"\"\"\n        # Optionally resize inputs if needed.\n        if self.resize_layer is not None:\n            inputs = self.resize_layer(inputs)\n        # Apply the appropriate normalization\n        inputs = self.preprocess_fn(inputs)\n        # inputs = self.cust_norm(inputs)\n        # Pass inputs through the backbone.\n        x = self.backbone(inputs, training=training)\n        # Apply GeM pooling.\n        x = self.pool(x)\n        # Flatten the pooled output.\n        x = self.flatten(x)\n        # Pass through the fully-connected layer and batch normalization.\n        x = self.fc(x)\n        x = self.bn(x, training=training)\n        # Apply the ArcMarginProduct layer.\n        x = self.arcMargin(x)\n        return x\n\n    # @tf.function\n    # def train_step(self, data):\n    #     x, y = data\n        \n    #     with tf.GradientTape() as tape:\n    #         y_pred = self(x, training=True)\n    #         loss = self.compute_loss(y=y, y_pred=y_pred)\n\n    #     # Compute gradients\n    #     if self.trainable_weights:\n    #         trainable_weights = self.trainable_weights\n    #         gradients = tape.gradient(loss, trainable_weights)\n\n    #         # Update weights\n    #         self.optimizer.apply_gradients(zip(gradients, trainable_weights))\n    #     else:\n    #         warnings.warn(\"The model does not have any trainable weights.\")\n\n    #     # Update the metrics.\n    #     # Metrics are configured in `compile()`.\n    #     for metric in self.metrics:\n    #         if metric.name == \"loss\":\n    #             metric.update_state(loss)\n    #         else:\n    #             metric.update_state(y, y_pred)\n        \n    #     # Return a dict mapping metric names to current value.\n    #     # Note that it will include the loss (tracked in self.metrics).\n    #     return {m.name: m.result() for m in self.metrics}\n        \n    # @tf.function\n    # def test_step(self, data):\n    #     x, y = data\n    #     y_pred = self(x, training=False)\n    #     # Updates the metrics tracking the loss\n    #     loss = self.compute_loss(y=y, y_pred=y_pred)\n    #     # Update all the metrics.\n    #     for metric in self.metrics:\n    #         if metric.name == \"loss\":\n    #             metric.update_state(loss)\n    #         else:\n    #             metric.update_state(y, y_pred)\n    #     # Return a dict mapping metric names to current value.\n    #     # Note that it will include the loss (tracked in self.metrics).\n    #     return {m.name: m.result() for m in self.metrics}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.493505Z","iopub.execute_input":"2025-04-01T18:25:55.493693Z","iopub.status.idle":"2025-04-01T18:25:55.512229Z","shell.execute_reply.started":"2025-04-01T18:25:55.493675Z","shell.execute_reply":"2025-04-01T18:25:55.511478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up KFold cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:25:55.533148Z","iopub.execute_input":"2025-04-01T18:25:55.533355Z","iopub.status.idle":"2025-04-01T18:25:55.547996Z","shell.execute_reply.started":"2025-04-01T18:25:55.533335Z","shell.execute_reply":"2025-04-01T18:25:55.547130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for fold_no, (train_index, val_index) in enumerate(kf.split(processed_images_paths, np.argmax(processed_images_labels, axis=1) if one_hot else processed_images_labels)): # Convert one-hot encoded labels to integer class labels for SKfold\n    print(f'Training on fold {fold_no}...')\n    \n    # Split the data into training and validation\n    train_paths, val_paths = processed_images_paths[train_index], processed_images_paths[val_index]\n    train_labels, val_labels = processed_images_labels[train_index], processed_images_labels[val_index]\n\n    # Create tf.data.Dataset objects\n    train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n\n    # Map the load_image function to load and preprocess the images\n    train_ds = train_ds.map(lambda img, label: preprocess_image(img, label, resize=True), num_parallel_calls=AUTOTUNE)\n    val_ds = val_ds.map(lambda img, label: preprocess_image(img, label, resize=True), num_parallel_calls=AUTOTUNE)\n\n    train_ds = configure_for_performance(train_ds, augment=True, shuffle=True, drop_remainder=True)\n    val_ds = configure_for_performance(val_ds, drop_remainder=False)\n    # Create a new model for each fold\n    with strategy.scope():\n        model = Model(backbone_name='xception', num_classes=num_classes, embedding_size=512)\n        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=schedule, weight_decay=5e-4),\n                      loss=ArcfaceLoss(one_hot=one_hot, smoothing=0.2),\n                      metrics=[tf.keras.metrics.CategoricalAccuracy('acc') if one_hot else tf.keras.metrics.SparseCategoricalAccuracy('acc')])\n    \n    history = model.fit(train_ds, validation_data=val_ds, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=EPOCHS, verbose=1)\n    \n    scores_orig = model.evaluate(orig_ds, verbose=0, return_dict=True)\n    print(f'Score for fold {fold_no} for all data: {scores_orig}')\n\n    if images_per_category:\n        scores_eval = model.evaluate(eval_ds, verbose=0, return_dict=True)\n        print(f'Score for fold {fold_no} for evaluation data: {scores_eval}')\n\n    # Test phase\n    predictions = []\n    predictions_tta = []\n    indices = []\n    \n    for batch_images, batch_indices in test_ds:\n        # Without TTA\n        batch_preds = model.predict(batch_images)\n        predictions = predictions + np.argmax(batch_preds, axis=1).tolist()\n        \n        tta_images_list = new_tta(batch_images)  # List of tensors with shape [B, H, W, C]\n        tta_preds = []\n        # Loop over each augmented image batch\n        for aug_images in tta_images_list:\n            preds = model.predict(aug_images, verbose=0)  # Each `preds` is of shape [B, num_classes]\n            tta_preds.append(preds)\n        tta_preds = np.mean(tta_preds, axis=0)\n        predictions_tta = predictions_tta + np.argmax(tta_preds, axis=1).tolist()\n        \n        indices.extend(batch_indices.numpy().astype('int32'))  # Collect indices to restore order later\n    predictions = [ x+1 for x in predictions ]\n    predictions_tta = [ x+1 for x in predictions_tta ]\n    \n    predictions_patches, predictions_patches_tta, indices_patches = patches_test(model, patch_test_ds)\n    # Convert predictions to DataFrame and save as CSV\n    df = pd.DataFrame({'case': indices, 'class': predictions})\n    df.to_csv(f'predictions{fold_no}.csv', index=False, sep=',')\n    # Convert predictions to DataFrame and save as CSV with TTA\n    df = pd.DataFrame({'case': indices, 'class': predictions_tta})\n    df.to_csv(f'predictions{fold_no}_TTA.csv', index=False, sep=',')","metadata":{"trusted":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2025-04-01T18:32:28.966031Z","iopub.execute_input":"2025-04-01T18:32:28.966391Z","execution_failed":"2025-04-01T18:38:27.942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}